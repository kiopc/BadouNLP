import json
import torch
import torch.nn as nn
import numpy as np
import math
import random
import os
import re
from transformers import BertTokenizer, BertModel
from torch.utils.data import Dataset, DataLoader

"""
基于Bert机构，进行sft形式的训练
"""

class LanguageModel(nn.Module):
    def __init__(self, hidden_size, vocab_size, pretrain_model_path):
        super(LanguageModel, self).__init__()
        self.bert = BertModel.from_pretrained(pretrain_model_path)
        self.classify = nn.Linear(hidden_size, vocab_size)
        self.loss = nn.CrossEntropyLoss(ignore_index=-1)

    def forward(self, x, mask=None, y=None):
        if y is not None:
            # 训练时，构建一个下三角的mask，让上下文之间没有交互
            print(mask.shape)
            output = self.bert(x, attention_mask=mask)
            x = output[0]
            y_pred = self.classify(x)  # output shape = (batch_size, vocab_size)
            return self.loss(y_pred.view(-1, y_pred.size(-1)), y.view(-1))
        else:
            # 预测时，不使用mask
            output = self.bert(x)
            x = output[0]
            y_pred = self.classify(x)
            return torch.softmax(y_pred, dim=-1)


# 加载语料，用title当作假想的propmt，content当作假想的target
def load_data(data_path):
    corpus = []
    with open(data_path, encoding='utf-8') as f:
        for line in f:
            line = json.loads(line)
            corpus.append([line["title"], line["content"]])
    return corpus

# sft数据构造
# loss只计算答案部分，通过mask矩阵，让上下文没有交互
# label中使用-1表示不参加训练
def build_dataset(tokenizer, corpus, max_length, batch_size):
    dataset = []
    for i, (prompt, target) in enumerate(corpus):
        prompt_encode = tokenizer.encode(prompt, add_special_tokens=False)
        target_encode = tokenizer.encode(target, add_special_tokens=False)
        x = [tokenizer.cls_token_id] + prompt_encode + [tokenizer.sep_token_id] + target_encode + [tokenizer.sep_token_id]
        y = len(prompt_encode) * [-1] + [-1] + target_encode + [tokenizer.sep_token_id] + [-1]
        # 构建mask矩阵，让prompt内可以交互，target中上下文没有交互
        mask = create_mask(len(prompt_encode), len(target_encode))
        # padding
        x = x[:max_length] + [0] * (max_length - len(x))
        y = y[:max_length] + [0] * (max_length - len(y))
        x = torch.LongTensor(x)
        y = torch.LongTensor(y)
        mask = pad_mask(mask, (max_length, max_length))
        dataset.append([x, mask, y])

    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)

# 构造掩码，输入两个字符串的长度
def create_mask(s1, s2):
    len_s1 = s1 + 2 # 加上cls和sep
    len_s2 = s2 + 1 # 加上sep
    # 创建掩码张量
    mask = torch.ones(len_s1 + len_s2, len_s1 + len_s2)
    # 遍历s1的每一个token
    for i in range(len_s1):
        mask[i, len_s1:] = 0
    # 遍历s2的每一个token
    for i in range(len_s2):
        # s2的token不能看到后面s2的token
        mask[i + len_s1, len_s1 + i + 1:] = 0
    return mask


def pad_mask(tensor, target_shape):
    # 获取输入张量和目标形状的长宽
    height, width = tensor.shape
    target_height, target_width = target_shape
    # 创建一个全零的目标张量
    result = torch.zeros(target_shape, dtype=tensor.dtype, device=tensor.device)
    # 计算需要填充或截断的区域
    h_start = 0
    w_start = 0
    h_end = min(height, target_height)
    w_end = min(width, target_width)
    # 将原始张量对应的部分填充到全0张量中
    result[h_start:h_end, w_start:w_end] = tensor[:h_end - h_start, :w_end - w_start]
    return result

# 建立模型
def build_model(vocab, char_dim, pretrain_model_path):
    model = LanguageModel(768, 21128, pretrain_model_path)
    return model

# 文本生成测试代码
def generate_sentence(openings, model, tokenizer):
    model.eval()
    openings = tokenizer.encode(openings)
    with torch.no_grad():
        # 生成文本超过50字则终止迭代
        while len(openings) <= 50:
            x = torch.LongTensor([openings])
            if torch.cuda.is_available():
                x = x.cuda()
            y = model(x)[0][-1]
            index = sampling_strategy(y)
            openings.append(index)
    return tokenizer.decode(openings)

# 采样策略
def sampling_strategy(prob_distribution):
    if random.random() > 0.1:
        strategy = "greedy"
    else:
        strategy = "sampling"
    if strategy == "greedy":
        return int(torch.argmax(prob_distribution))
    elif strategy == "sampling":
        prob_distribution = prob_distribution.cpu().numpy()
        return np.random.choice(list(range(len(prob_distribution))), p=prob_distribution)


def main(corpus_path, save_weight=True):
    epoch_num = 20        #训练轮数
    batch_size = 32       #每次训练样本个数
    char_dim = 768        #每个字的维度
    max_length = 50       #样本文本长度
    vocab_size = 21128      #字表大小
    learning_rate = 0.001  #学习率

    pretrain_model_path = "bert-base-chinese"
    tokenizer = BertTokenizer.from_pretrained(pretrain_model_path)

    corpus = load_data(corpus_path)  # 加载语料
    train_data = build_dataset(tokenizer, corpus, max_length, batch_size)  # 构建数据集
    model = build_model(vocab_size, char_dim, pretrain_model_path)  # 构建模型
    if torch.cuda.is_available():
        model = model.cuda()
    optim = torch.optim.Adam(model.parameters(), lr=learning_rate)  # 优化器
    print("文本词表模型加载完毕，开始训练")
    for epoch in range(epoch_num):
        model.train()
        watch_loss = []
        for x, mask, y in train_data:  # 构建一组训练样本
            if torch.cuda.is_available():
                x = x.cuda()
                mask = mask.cuda()
                y = y.cuda()
            optim.zero_grad()  # 梯度归零
            loss = model(x, mask, y)  # 计算loss
            loss.backward()  # 反向传播
            optim.step()  # 更新权重
            watch_loss.append(loss.item())
        print("=========\n第%d轮平均loss:%f" % (epoch + 1, np.mean(watch_loss)))
        print(generate_sentence("北京明年拟推工作日半价观看电影", model, tokenizer))
        print(generate_sentence("南京一合金厂锅炉发生爆炸", model, tokenizer))

    if not save_weight:
        return 
    else:
        base_name = os.path.basename(corpus_path).replace("txt", "pth")
        model_path = os.path.join("model", base_name)
        torch.save(model.state_dict(), model_path)
        return
    
if __name__ == "__main__":
    main("sample_data.json", False)
